{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import glob\n",
    "\n",
    "import tensorflow.keras.preprocessing.image\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow.keras.preprocessing.image\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input, layers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "HEIGHT=224\n",
    "WIDTH=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "encode_model = VGG16()\n",
    "print(encode_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "  \n",
    "def findsubsets(s, n): \n",
    "    return list(itertools.combinations(s, n)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = 'this is a test sentence'\n",
    "candidate = 'this is also a test sentence'\n",
    "print([reference.split()])\n",
    "print(candidate.split())\n",
    "\n",
    "score = sentence_bleu([reference.split()], candidate.split())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestCaption(candidateCaptions, m):\n",
    "    bestNow = ''\n",
    "    maxSim = 0\n",
    "    \n",
    "    if len(candidateCaptions) < m:\n",
    "        m = len(candidateCaptions)\n",
    "    \n",
    "    for caption1 in candidateCaptions:\n",
    "        lis = []\n",
    "        Reference = [caption1]\n",
    "        for idx in range(len(candidateCaptions)):\n",
    "            lis.append(nltk.translate.bleu_score.sentence_bleu(Reference, candidateCaptions[idx]))\n",
    "        lis.sort()\n",
    "        lis = lis[-m:]\n",
    "        sim = sum(lis)\n",
    "        if sim > maxSim:\n",
    "            bestNow = caption1\n",
    "            maxSim = sim\n",
    "        \n",
    "    return [bestNow, maxSim/(5*m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepKNN(img_id):\n",
    "    return KNN(encoding_test[img_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepKNNdev(img_id):\n",
    "    return KNN(encoding_dev[img_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaptiveKNN(feature_vector, trained_data, captionMap, k, m, threshold):\n",
    "    similarityList = []\n",
    "    \n",
    "    for img_id in trained_data:\n",
    "        sim = cosine_similarity(feature_vector, trained_data[img_id])\n",
    "        similarityList.append((sim, img_id))\n",
    "    \n",
    "    similarityList.sort()\n",
    "    similarityList = similarityList[-k:]\n",
    "    adaptiveList = []\n",
    "    adaptiveList.append(similarityList[k-1])\n",
    "    \n",
    "    for i in range(k-2, -1, -1):\n",
    "        if similarityList[i + 1][0] - similarityList[i][0] < threshold:\n",
    "            break;\n",
    "        adaptiveList.append(similarityList[i])\n",
    "    \n",
    "    candidateCaptions = []\n",
    "    for pair in adaptiveList:\n",
    "        for caption in captionMap[pair[1]]:\n",
    "            candidateCaptions.append(caption.split())\n",
    "           \n",
    "    return bestCaption(candidateCaptions, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = \"D:\\\\flickr8k\\\\Flickr8k\"\n",
    "rc='D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset'\n",
    "img = glob.glob(os.path.join(rc, '*.jpg'))\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATASET\n",
    "train_images_path = os.path.join(root_captioning,'Flickr8k_text','Flickr_8k.trainImages.txt') \n",
    "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
    "test_images_path = os.path.join(root_captioning,'Flickr8k_text','Flickr_8k.testImages.txt') \n",
    "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
    "\n",
    "dev_images_path = os.path.join(root_captioning,'Flickr8k_text','Flickr_8k.devImages.txt') \n",
    "dev_images = set(open(dev_images_path, 'r').read().strip().split('\\n'))\n",
    "\n",
    "train_img = []\n",
    "test_img = []\n",
    "dev_img=[]\n",
    "\n",
    "for i in img:\n",
    "  f = os.path.split(i)[-1]\n",
    "  if f in train_images: \n",
    "    train_img.append(f) \n",
    "  elif f in test_images:\n",
    "    test_img.append(f)\n",
    "  else:\n",
    "    dev_img.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_punct = str.maketrans('', '', string.punctuation)\n",
    "lookup = dict()\n",
    "root_captioning = \"D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_text\"\n",
    "\n",
    "with open( os.path.join(root_captioning,'','Flickr8k.token.txt'), 'r') as fp:\n",
    "  \n",
    "  max_length = 0\n",
    "  for line in fp.read().split('\\n'):\n",
    "    tok = line.split()\n",
    "    if len(line) >= 2:\n",
    "      id = tok[0].split('.')[0]\n",
    "      id = id + '.jpg'\n",
    "      desc = tok[1:]\n",
    "      listToStr = ' '.join([str(elem) for elem in desc])   \n",
    "#       # Cleanup description\n",
    "#       desc = [word.lower() for word in desc]  #lowercase all words\n",
    "#       desc = [w.translate(null_punct) for w in desc] # remove punctuations\n",
    "#       desc = [word for word in desc if len(word)>1] #remove small words\n",
    "#       desc = [word for word in desc if word.isalpha()]  #remove all digits\n",
    "#       max_length = max(max_length,len(desc)) #calculate the max length of the caption\n",
    "      \n",
    "      if id not in lookup:\n",
    "        lookup[id] = list()\n",
    "      lookup[id].append(listToStr)\n",
    "    \n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "def encodeImage(img):\n",
    "  # Resize all images to a standard size (specified bythe image encoding network)\n",
    "  img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)\n",
    "  # Convert a PIL image to a numpy array\n",
    "  x = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
    "  # reshape data for the model\n",
    "  x = x.reshape((1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "  # Perform any preprocessing needed by InceptionV3 or others\n",
    "  x = preprocess_input(x)\n",
    "    \n",
    "  get_fc_layer_output = K.function([encode_model.layers[0].input], [encode_model.layers[-2].output])\n",
    "  layer_output = get_fc_layer_output([x])[0]\n",
    "  \n",
    "  return layer_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = \"D:\\\\flickr8k\\\\Flickr8k\"\n",
    "train_path = os.path.join(root_captioning,\"data\",f'train.pkl')\n",
    "if not os.path.exists(train_path):\n",
    "  start = time()\n",
    "  encoding_train = {}\n",
    "  for id in train_img:\n",
    "    image_path = os.path.join('E:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flicker8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "    encoding_train[id] = encodeImage(img)\n",
    "  with open(train_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_train, fp)\n",
    "  \n",
    "else:\n",
    "  with open(train_path, \"rb\") as fp:\n",
    "    encoding_train = pickle.load(fp)\n",
    "\n",
    "# print(encoding_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(encoding_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091\n"
     ]
    }
   ],
   "source": [
    "root_captioning = \"D:\\\\flickr8k\\\\Flickr8k\"\n",
    "dev_path = os.path.join(root_captioning,\"data\",f'dev.pkl')\n",
    "if not os.path.exists(train_path):\n",
    "  start = time()\n",
    "  encoding_dev = {}\n",
    "  for id in dev_img:\n",
    "    image_path = os.path.join('D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "    encoding_dev[id] = encodeImage(img)\n",
    "  with open(dev_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_dev, fp)\n",
    "  \n",
    "else:\n",
    "  with open(dev_path, \"rb\") as fp:\n",
    "    encoding_dev = pickle.load(fp)\n",
    "\n",
    "print( len(encoding_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def KNN(feature_vector, trained_data = encoding_train, captionMap = lookup, k = 30, m = 50):\n",
    "    similarityList = []\n",
    "    \n",
    "    for img_id in trained_data:\n",
    "        sim = cosine_similarity(feature_vector, trained_data[img_id])\n",
    "        similarityList.append((sim, img_id))\n",
    "    \n",
    "    similarityList.sort()\n",
    "    similarityList = similarityList[-k:]\n",
    "    simScore = 0\n",
    "    for tup in similarityList:\n",
    "        simScore += tup[0]\n",
    "        \n",
    "    candidateCaptions = []\n",
    "    for pair in similarityList:\n",
    "        for caption in captionMap[pair[1]]:\n",
    "            candidateCaptions.append(caption.split())\n",
    "           \n",
    "    return [bestCaption(candidateCaptions, m), simScore/(5*k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajulr\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\ajulr\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'dogs', 'are', 'playing', 'in', 'the', 'snow', '.'], 0.01356206629354431], 0.09145502110322316]\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join('D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset', '667626_18933d713e'+'.jpg')\n",
    "img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "# img.show()\n",
    "print(KNN(encodeImage(img), encoding_train, lookup, 30, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# encode all test images\n",
    "\n",
    "root_captioning = \"D:\\\\flickr8k\\\\Flickr8k\"\n",
    "test_path = os.path.join(root_captioning,\"data\",f'test.pkl')\n",
    "if not os.path.exists(test_path):\n",
    "  start = time()\n",
    "  encoding_test = {}\n",
    "  for id in test_img:\n",
    "    image_path = os.path.join('E:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flicker8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "    encoding_test[id] = encodeImage(img)\n",
    "  with open(test_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_test, fp)\n",
    "  \n",
    "else:\n",
    "  with open(test_path, \"rb\") as fp:\n",
    "    encoding_test = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(encoding_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['A', 'man', 'and', 'woman', 'wearing', 'Mickey', 'Mouse', 'ears', 'stand', 'in', 'a', 'crowd', '.'], 0.010181503126359886], 0.08892701824506123]\n",
      "[[['A', 'man', 'and', 'woman', 'wearing', 'Mickey', 'Mouse', 'ears', 'stand', 'in', 'a', 'crowd', '.'], 0.010181503126359886]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c426e9982487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictedCaptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbleu_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroundTruth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictedCaptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py\u001b[0m in \u001b[0;36msentence_bleu\u001b[1;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m    105\u001b[0m     return corpus_bleu(\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mreferences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmoothing_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_reweigh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;31m# denominator for the corpus-level modified precision.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mp_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodified_precision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[0mp_numerators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mp_denominators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py\u001b[0m in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;31m# Extracts all ngrams in hypothesis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;31m# Set an empty Counter if hypothesis is empty.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;31m# Extract a union of references' counts.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;31m# max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    620\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#from ipynb.fs.full.lstm_caption_model import factorial\n",
    "# generate captions for all images \n",
    "# build groud truth caption array\n",
    "K =  30\n",
    "M = 50\n",
    "\n",
    "predictedCaptions = []\n",
    "groundTruth = []\n",
    "\n",
    "ct = 0\n",
    "for img_id in test_img:\n",
    "    if ct > 22:\n",
    "        break\n",
    "    feature = encoding_test[img_id]\n",
    "    caption = KNN(feature, encoding_train, lookup, K, M)\n",
    "    lis = lookup[img_id]\n",
    "    references = []\n",
    "    for i in range(len(lis)):\n",
    "        references.append(lis[i].split()) \n",
    "    predictedCaptions.append(caption[0])\n",
    "    groundTruth.append(references)\n",
    "    print(caption)\n",
    "    print(predictedCaptions)\n",
    "    print(nltk.translate.bleu_score.sentence_bleu(groundTruth[ct], predictedCaptions[ct]))\n",
    "    ct += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, predictedCaptions, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, predictedCaptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import tensorflow.keras.applications.inception_v3\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow.keras.preprocessing.image\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Embedding,Dense,Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input, layers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EPOCHS = 5\n",
    "USE_INCEPTION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_punct = str.maketrans('', '', string.punctuation)\n",
    "lookup2 = dict()\n",
    "root_captioning = \"D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_text\"\n",
    "\n",
    "with open( os.path.join(root_captioning,'Flickr8k.token.txt'), 'r') as fp:\n",
    "  \n",
    "  max_length = 0\n",
    "  for line in fp.read().split('\\n'):\n",
    "    tok = line.split()\n",
    "    if len(line) >= 2:\n",
    "      id = tok[0].split('.')[0]\n",
    "      desc = tok[1:]\n",
    "      \n",
    "      # Cleanup description\n",
    "      desc = [word.lower() for word in desc]  #lowercase all words\n",
    "      desc = [w.translate(null_punct) for w in desc] # remove punctuations\n",
    "#       desc = [word for word in desc if len(word)>1] #remove small words\n",
    "      desc = [word for word in desc if word.isalpha()]  #remove all digits\n",
    "      max_length = max(max_length,len(desc)) #calculate the max length of the caption\n",
    "      \n",
    "      if id not in lookup2:\n",
    "        lookup2[id] = list()\n",
    "      lookup2[id].append(' '.join(desc))\n",
    "      \n",
    "lex = set()\n",
    "for key in lookup2:\n",
    "  [lex.update(d.split()) for d in lookup2[key]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8092\n",
      "8775\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(lookup2)) # How many unique words\n",
    "print(len(lex)) # The dictionary\n",
    "print(max_length) # Maximum length of a caption (in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset'\n",
    "img = glob.glob(os.path.join(root_captioning, '*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_text'\n",
    "train_images_path = os.path.join(root_captioning,'Flickr_8k.trainImages.txt') \n",
    "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
    "test_images_path = os.path.join(root_captioning,'Flickr_8k.testImages.txt') \n",
    "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
    "dev_images_path = os.path.join(root_captioning,'Flickr_8k.devImages.txt') \n",
    "dev_images = set(open(dev_images_path, 'r').read().strip().split('\\n'))\n",
    "\n",
    "\n",
    "train_img = []\n",
    "test_img = []\n",
    "dev_img=[]\n",
    "for i in img:\n",
    "  f = os.path.split(i)[-1]\n",
    "  if f in train_images: \n",
    "    train_img.append(f) \n",
    "  elif f in test_images:\n",
    "    test_img.append(f) \n",
    "  else:\n",
    "    dev_img.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_images))\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = {k:v for k,v in lookup2.items() if f'{k}.jpg' in train_images}\n",
    "for n,v in train_descriptions.items(): \n",
    "  for d in range(len(v)):\n",
    "    v[d] = f'{START} {v[d]} {STOP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_INCEPTION:\n",
    "  encode_model = InceptionV3(weights='imagenet')\n",
    "  encode_model = Model(encode_model.input, encode_model.layers[-2].output)\n",
    "  WIDTH = 299\n",
    "  HEIGHT = 299\n",
    "  OUTPUT_DIM = 2048\n",
    "  preprocess_input = tensorflow.keras.applications.inception_v3.preprocess_input\n",
    "else:\n",
    "  encode_model = MobileNet(weights='imagenet',include_top=False)\n",
    "  WIDTH = 224\n",
    "  HEIGHT = 224\n",
    "  OUTPUT_DIM = 50176\n",
    "  preprocess_input = tensorflow.keras.applications.mobilenet.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeImagelstm(img):\n",
    "  # Resize all images to a standard size (specified bythe image encoding network)\n",
    "  img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)\n",
    "  # Convert a PIL image to a numpy array\n",
    "  x = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
    "  # Expand to 2D array\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  # Perform any preprocessing needed by InceptionV3 or others\n",
    "  x = preprocess_input(x)\n",
    "  # Call InceptionV3 (or other) to extract the smaller feature set for the image.\n",
    "  x = encode_model.predict(x) # Get the encoding vector for the image\n",
    "  # Shape to correct form to be accepted by LSTM captioning network.\n",
    "\n",
    "  x = np.reshape(x, x.shape[1] )\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\lstmdata'\n",
    "train_path = os.path.join(root_captioning,f'train{OUTPUT_DIM}.pkl')\n",
    "if not os.path.exists(train_path):\n",
    "  start = time()\n",
    "  encoding_trainlstm = {}\n",
    "  for id in tqdm(train_img):\n",
    "    image_path = os.path.join(root_captioning,'Images', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "    encoding_trainlstm[id] = encodeImagelstm(img)\n",
    "  with open(train_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_trainlstm, fp)\n",
    "  print(f\"\\nGenerating training set took: {hms_string(time()-start)}\")\n",
    "else:\n",
    "  with open(train_path, \"rb\") as fp:\n",
    "    encoding_trainlstm = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\lstmdata'\n",
    "test_path = os.path.join(root_captioning,f'test{OUTPUT_DIM}.pkl')\n",
    "if not os.path.exists(test_path):\n",
    "  start = time()\n",
    "  encoding_testlstm = {}\n",
    "  for id in tqdm(test_img):\n",
    "    image_path = os.path.join('D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "    encoding_testlstm[id] = encodeImagelstm(img)\n",
    "  with open(test_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_testlstm, fp)\n",
    "else:\n",
    "  with open(test_path, \"rb\") as fp:\n",
    "    encoding_testlstm = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\lstmdata'\n",
    "dev_path = os.path.join(root_captioning,f'dev{OUTPUT_DIM}.pkl')\n",
    "if not os.path.exists(dev_path):\n",
    "  start = time()\n",
    "  encoding_devlstm = {}\n",
    "  for id in tqdm(dev_img):\n",
    "    image_path = os.path.join('D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, target_size=(HEIGHT, WIDTH))\n",
    "    encoding_devlstm[id] = encodeImagelstm(img)\n",
    "  with open(dev_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_devlstm, fp)\n",
    "else:\n",
    "  with open(dev_path, \"rb\") as fp:\n",
    "    encoding_devlstm = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "len(all_train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 7589 ==> 2533\n"
     ]
    }
   ],
   "source": [
    "word_count_threshold = 5\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2534"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoidx[w] = ix\n",
    "    idxtoword[ix] = w\n",
    "    ix += 1\n",
    "    \n",
    "vocab_size = len(idxtoword) + 1 \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "max_length +=2\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "def data_generator2(descriptions, photos, wordtoidx, max_length, num_photos_per_batch):\n",
    "  # x1 - Training data for photos\n",
    "  # x2 - The caption that goes with each photo\n",
    "  # y - The predicted rest of the caption\n",
    "  x1, x2, y = [], [], []\n",
    "  n=0\n",
    "  while True:\n",
    "    for key, desc_list in descriptions.items():\n",
    "      n+=1\n",
    "      photo = photos[key+'.jpg']\n",
    "      # Each photo has 5 descriptions\n",
    "      for desc in desc_list:\n",
    "        # Convert each word into a list of sequences.\n",
    "        seq = [wordtoidx[word] for word in desc.split(' ') if word in wordtoidx]\n",
    "        # Generate a training case for every possible sequence and outcome\n",
    "        for i in range(1, len(seq)):\n",
    "          in_seq, out_seq = seq[:i], seq[i]\n",
    "          in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "          x1.append(photo)\n",
    "          x2.append(in_seq)\n",
    "          y.append(out_seq)\n",
    "      if n==num_photos_per_batch:\n",
    "        yield ([np.array(x1), np.array(x2)], np.array(y))\n",
    "        x1, x2, y = [], [], []\n",
    "        n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\flickr8k\\\\Flickr8k\\\\lstmdata\\\\glove.6B.200d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d8a8e4ba2db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mglove_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_captioning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'glove.6B.200d.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\flickr8k\\\\Flickr8k\\\\lstmdata\\\\glove.6B.200d.txt'"
     ]
    }
   ],
   "source": [
    "root_captioning = 'E:\\\\flickr8k\\\\Flickr8k\\\\lstmdata'\n",
    "glove_dir = os.path.join(root_captioning)\n",
    "embeddings_index = {} \n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in wordtoidx.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2534, 200)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(OUTPUT_DIM,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 37)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 37, 200)      506800      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 37, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2534)         651238      dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,216,342\n",
      "Trainable params: 2,216,342\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.layers[2].set_weights([embedding_matrix])\n",
    "caption_model.layers[2].trainable = False\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_pics_per_bath = 3\n",
    "steps = len(train_descriptions)/number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n"
     ]
    }
   ],
   "source": [
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\lstmdata'\n",
    "model_path = os.path.join(root_captioning,f'caption-modelacctopaper.hdf5')\n",
    "start= time()\n",
    "print(caption_model.optimizer.lr)\n",
    "if not os.path.exists(model_path):\n",
    "  for i in tqdm(range(EPOCHS)):\n",
    "      steps= int(steps)\n",
    "      generator = data_generator2(train_descriptions, encoding_trainlstm, wordtoidx, max_length, number_pics_per_bath)\n",
    "      caption_model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n",
    "  caption_model.optimizer.lr = 1e-4\n",
    "  number_pics_per_bath = 6\n",
    "  steps = len(train_descriptions)//number_pics_per_bath\n",
    "\n",
    "  for i in range(EPOCHS):\n",
    "      generator = data_generator2(train_descriptions, encoding_trainlstm, wordtoidx, max_length, number_pics_per_bath)\n",
    "      caption_model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)  \n",
    "  caption_model.save_weights(model_path)\n",
    "  print(f\"\\Training took: {hms_string(time()-start)}\")\n",
    "else:\n",
    "  caption_model.load_weights(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCaption(photo):\n",
    "    in_text = START\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = caption_model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = idxtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == STOP:\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    print('Greedy Search: ' +final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCaptionBeamSearchModified(photo,k):\n",
    "    t=k\n",
    "    new_sentences=[ [START,0]]    #array of sentence log probability pairs \n",
    "    while True:\n",
    "        \n",
    "        \n",
    "        new_sentences = sorted(new_sentences, key=lambda tup:tup[1], reverse=True)\n",
    "        \n",
    "        \n",
    "        new_sentences= new_sentences[:k]\n",
    "\n",
    "#         print(' ')\n",
    "#         print('...........................................................')\n",
    "        end =True\n",
    "        \n",
    "        for x in new_sentences:\n",
    "            if x[0].split()[-1]!=STOP and len(x[0].split())<max_length:\n",
    "                end=False\n",
    "                    \n",
    "      \n",
    "        if end==True:\n",
    "            new_sentences = sorted(new_sentences,key=lambda tup:tup[1],reverse=True)\n",
    "            break\n",
    "        \n",
    "        possible_sentences= new_sentences\n",
    "        new_sentences=[]\n",
    "        \n",
    "        \n",
    "        for x in possible_sentences:\n",
    "            \n",
    "            in_text = x[0]\n",
    "            if in_text.split()[-1]==STOP:\n",
    "                new_sentences.append(x)\n",
    "                continue\n",
    "                \n",
    "            sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "#             print(sequence)\n",
    "            yhat = caption_model.predict([photo,sequence], verbose=0)\n",
    "            \n",
    "           \n",
    "            new_probs=[]\n",
    "            for i in range (len(yhat[0])):\n",
    "                if(yhat[0][i]!=0):\n",
    "                    yhat[0][i]=np.log(yhat[0][i])\n",
    "                    new_probs.append( [i, x[1] + yhat[0][i]])\n",
    "\n",
    "#             ordered = sorted(new_probs, key=lambda tup:tup[1],reverse=True)\n",
    "            for y in new_probs:\n",
    "                if(y[0]!=0):\n",
    "                    z=in_text +' '+ idxtoword[y[0]]\n",
    "                    new_sentences.append([z,y[1]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    ns=new_sentences\n",
    "    for x in ns:\n",
    "        x[1]= x[1] / len(x[0].split())\n",
    "    ns = sorted(ns, key=lambda tup:tup[1] , reverse=True)\n",
    "    \n",
    "#     print(ns)\n",
    "    \n",
    "#     for x in range(len(ns)):\n",
    "#         print(ns[x][0])\n",
    "#         print(new_sentences[x][0])\n",
    "#     print('=----------------------')\n",
    "#     print(new_sentences)\n",
    "        \n",
    "    \n",
    "#     print(ns)\n",
    "    complete = []\n",
    "    for tup in ns:\n",
    "        y=tup[0].split()[1:-1] \n",
    "        y.append('.')\n",
    "        y[0]=y[0].capitalize()\n",
    "        complete.append(y)\n",
    "#     print('Beam search( k =' +str(t) +') : ',complete[0])\n",
    "    return [complete[0], ns[0][1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def generateCaptionBeamSearch(photo,k):\n",
    "    t=k\n",
    "    new_sentences=[ [START,1.0]]    #array of sentence probability pairs \n",
    "    while True:\n",
    "        \n",
    "        new_sentences = sorted(new_sentences, key=lambda tup:tup[1])\n",
    "        \n",
    "        new_sentences= new_sentences[:k]\n",
    "#         print(new_sentences)\n",
    "#         print(' ')\n",
    "#         print('...........................................................')\n",
    "        end =True\n",
    "        \n",
    "        for x in new_sentences:\n",
    "            if x[0].split()[-1]!=STOP:\n",
    "                end=False\n",
    "                    \n",
    "                \n",
    "        if end==True:\n",
    "            new_sentences = sorted(new_sentences, key=lambda tup:tup[1])\n",
    "            break\n",
    "        \n",
    "        possible_sentences= new_sentences\n",
    "        new_sentences=[]\n",
    "        \n",
    "        \n",
    "        for x in possible_sentences:\n",
    "            \n",
    "            in_text = x[0]\n",
    "            if in_text.split()[-1]==STOP:\n",
    "                new_sentences.append(x)\n",
    "                continue\n",
    "                \n",
    "            sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "#             print(sequence)\n",
    "            yhat = caption_model.predict([photo,sequence], verbose=0)\n",
    "#             print(len(yhat[0]))\n",
    "            new_probs=[]\n",
    "            for i in range (len(yhat[0])):\n",
    "                yhat[0][i]=-np.log(yhat[0][i])\n",
    "                new_probs.append( [i, x[1] + yhat[0][i]])\n",
    "\n",
    "            ordered = sorted(new_probs, key=lambda tup:tup[1])\n",
    "            ordered= ordered[:k]\n",
    "            for y in ordered:\n",
    "                if(y[0]!=0):\n",
    "                    z=in_text +' '+ idxtoword[y[0]]\n",
    "                    new_sentences.append([z,y[1]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print(new_sentences)\n",
    "    final = new_sentences[0][0].split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    print('Beam search( k =' +str(t) +') : '+final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "#from ipynb.fs.full.KNNModel import prepKNN\n",
    "# reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "# candidate = ['this', 'is', 'a', 'test']\n",
    "# score = sentence_bleu(reference, candidate)\n",
    "# print(score)\n",
    "\n",
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset'\n",
    "# print(lookup)\n",
    "pc1=[]\n",
    "pc3=[]\n",
    "pc5=[]\n",
    "pc7=[]\n",
    "pcbest=[]\n",
    "groundTruth = []\n",
    "for z in tqdm(range(1,1000)):\n",
    "    \n",
    "  \n",
    "  pic = list(encoding_testlstm.keys())[z]\n",
    "  #print(lookup[pic[:len(pic)-4]])\n",
    "\n",
    "  image = encoding_testlstm[pic].reshape((1,OUTPUT_DIM))\n",
    "\n",
    "#   cs1=generateCaptionBeamSearchModified(image,1)[0]\n",
    "# #   print(cs1)\n",
    "#   cs3=generateCaptionBeamSearchModified(image,3)[0]\n",
    "# #   print(cs3)\n",
    "#   cs5=generateCaptionBeamSearchModified(image,5)[0]\n",
    "# #   print(cs5)\n",
    "  cs7=generateCaptionBeamSearchModified(image,7)[0]\n",
    "\n",
    "#   pc1.append(cs1)\n",
    "#   pc3.append(cs3)\n",
    "#   pc5.append(cs5)\n",
    "  pc7.append(cs7)\n",
    "\n",
    "\n",
    "    \n",
    "  references=[]\n",
    "\n",
    "  reference=lookup[pic]\n",
    "  for i in reference:\n",
    "      rs=i.split()\n",
    "      references.append(rs)\n",
    "\n",
    "    \n",
    "  groundTruth.append(references)\n",
    "\n",
    "# print(\"scoregreedy\")\n",
    "    \n",
    "    \n",
    "# print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc1, weights=(1, 0, 0, 0)))\n",
    "# print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc1))\n",
    "\n",
    "# print(\"score k=3\")\n",
    "# print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc3, weights=(1, 0, 0, 0)))\n",
    "# print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc3))\n",
    "\n",
    "\n",
    "# print(\"score k=5\")\n",
    "# print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc5, weights=(1, 0, 0, 0)))\n",
    "# print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc5))\n",
    "\n",
    "print(\"score k=7\")\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc7, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc7))\n",
    "\n",
    "# print( bsg/count)\n",
    "# print(bsb/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoregreedy\n",
      "0.5759066740734863\n",
      "0.14448463625512478\n",
      "score k=3\n",
      "0.5813370909660247\n",
      "0.16012126098220594\n",
      "score k=5\n",
      "0.5809864633577096\n",
      "0.16290733291148451\n"
     ]
    }
   ],
   "source": [
    "print(\"scoregreedy\")\n",
    "    \n",
    "    \n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc1, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc1))\n",
    "\n",
    "print(\"score k=3\")\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc3, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc3))\n",
    "\n",
    "\n",
    "print(\"score k=5\")\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc5, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for testing lstm\n",
    "\n",
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset'\n",
    "\n",
    "pc2=[]\n",
    "pcbest=[]\n",
    "groundTruth = []\n",
    "bsg=0\n",
    "bsb=0\n",
    "count=0\n",
    "for z in tqdm(range(1,2)):\n",
    "    \n",
    "  count+=1\n",
    "  pic = list(encoding_testlstm.keys())[z]\n",
    "  #print(lookup[pic[:len(pic)-4]])\n",
    "\n",
    "  image = encoding_testlstm[pic].reshape((1,OUTPUT_DIM))\n",
    "#   print(os.path.join(root_captioning, pic))\n",
    "#   x=plt.imread(os.path.join(root_captioning, pic))\n",
    "#   plt.imshow(x)\n",
    "#   plt.show()\n",
    "#   print(\"Caption:\",generateCaptionbs(image))\n",
    "\n",
    "  cs2=generateCaptionBeamSearchModified(image,30)\n",
    "  cs2 = cs2[0]\n",
    "  pc2.append(cs2)\n",
    "  references=[]\n",
    "  \n",
    "  reference=lookup[pic]\n",
    "  \n",
    "  for i in reference:\n",
    "      rs=i.split()\n",
    "      references.append(rs)\n",
    " \n",
    "  \n",
    "#   if(val > 0):\n",
    "#     print('LSTM wins')\n",
    "#   print('similarity',captionFromKNN[1], logprob , 'diff: ',val)\n",
    "    \n",
    "    \n",
    "  groundTruth.append(references)\n",
    "print(\"score lstm\")\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc2, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc2))\n",
    "\n",
    "# print( bsg/count)\n",
    "\n",
    "# print(bsb/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for 1000 images \n",
    "k=3\n",
    "score lstm\n",
    "0.5813370909660247\n",
    "0.16012126098220594\n",
    "\n",
    "\n",
    "for 10 images \n",
    "\n",
    "k=1\n",
    "score lstm\n",
    "0.5849056603773585\n",
    "0.13977009191061607\n",
    "\n",
    "\n",
    "k=5\n",
    "score lstm\n",
    "0.5483870967741935\n",
    "0.22861607833050931\n",
    "\n",
    "k=3\n",
    "score lstm\n",
    "0.5867768595041323\n",
    "0.22920021114649844\n",
    "\n",
    "k=10\n",
    "score lstm\n",
    "0.5461538461538461\n",
    "0.24651229669718183\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for 20 imges\n",
    "k=1\n",
    "score lstm\n",
    "0.5578512396694215\n",
    "0.11388743593664953\n",
    "\n",
    "k=3\n",
    "score lstm\n",
    "0.5691056910569106\n",
    "0.18859232211707525\n",
    "\n",
    "k=5\n",
    "score lstm\n",
    "0.54296875\n",
    "0.18068871906408002\n",
    "\n",
    "k=7\n",
    "score lstm\n",
    "0.5408560311284046\n",
    "0.19010217384382072\n",
    "\n",
    "k=10\n",
    "score lstm\n",
    "0.528957528957529\n",
    "0.17082489617419983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put(x_train, y_train, logisticFeatures, lstmBetter, val):\n",
    "    if val >= 0.1:\n",
    "        for i in range(5):\n",
    "            x_train.append(logisticFeatures)\n",
    "            y_train.append(lstmBetter)\n",
    "    elif val >= 0.01:\n",
    "         for i in range(3):\n",
    "            x_train.append(logisticFeatures)\n",
    "            y_train.append(lstmBetter)   \n",
    "    elif val >= 0.0001:\n",
    "        for i in range(2):\n",
    "            x_train.append(logisticFeatures)\n",
    "            y_train.append(lstmBetter)\n",
    "    elif val >= 0.00000001:\n",
    "        for i in range(1):\n",
    "            x_train.append(logisticFeatures)\n",
    "            y_train.append(lstmBetter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1091 [00:00<?, ?it/s]C:\\Users\\ajulr\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1091/1091 [1:22:52<00:00,  4.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "#from ipynb.fs.full.KNNModel import prepKNN\n",
    "# reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "# candidate = ['this', 'is', 'a', 'test']\n",
    "# score = sentence_bleu(reference, candidate)\n",
    "# print(score)\n",
    "\n",
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset'\n",
    "# print(lookup)\n",
    "pc1=[]\n",
    "pc2=[]\n",
    "x_train = []\n",
    "y_train = []\n",
    "groundTruth = []\n",
    "\n",
    "bsg=0\n",
    "bsb=0\n",
    "count=0\n",
    "dev_size = len(encoding_devlstm.keys())\n",
    "print(dev_size)\n",
    "for z in tqdm(range(dev_size)):\n",
    "  count+=1\n",
    "\n",
    "  pic = list(encoding_devlstm.keys())[z]\n",
    "\n",
    "  image = encoding_devlstm[pic].reshape((1,OUTPUT_DIM))\n",
    "#   print(os.path.join(root_captioning, pic))\n",
    "  x=plt.imread(os.path.join(root_captioning, pic))\n",
    "  #plt.imshow(x)\n",
    "  #plt.show()\n",
    "#   print(\"Caption:\",generateCaptionbs(image))\n",
    "\n",
    "  captionFromKNN = prepKNNdev(pic)\n",
    "#   print(captionFromKNN)\n",
    "  #cs= generateCaptionBeamSearch(image,30)\n",
    "  cs2=generateCaptionBeamSearchModified(image,3)\n",
    "  #cs3=propModel(cs2, captionFromKNN)\n",
    "\n",
    "  probab = cs2[1]\n",
    "  cs2 = cs2[0]\n",
    "\n",
    "  pc1.append(captionFromKNN[0][0])\n",
    "  pc2.append(cs2)\n",
    "  \n",
    "  references=[]\n",
    "  score=0\n",
    "  \n",
    "  reference=lookup[pic]\n",
    "  for i in reference:\n",
    "      rs=i.split()\n",
    "      references.append(rs)\n",
    "#   print(references)\n",
    "  val = -sentence_bleu(references,captionFromKNN[0][0] ) + sentence_bleu(references,cs2)\n",
    "  \n",
    "  lstmBetter = 0\n",
    "  if val > 0:\n",
    "        lstmBetter = 1\n",
    "  \n",
    "  logisticFeatures = [probab, captionFromKNN[0][1], captionFromKNN[1], len(captionFromKNN[0][0])/32, len(cs2)/32]      \n",
    "  put(x_train, y_train, logisticFeatures, lstmBetter, abs(val))\n",
    "    \n",
    "  groundTruth.append(references)  \n",
    " \n",
    "  \n",
    "#   print(\"_____________________________________\")\n",
    "# print( bsg/count)\n",
    "# print(bsb/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc1copy=pc1\n",
    "# pc2copy=pc2\n",
    "# xtraincopy=x_train\n",
    "# ytraincopy=y_train\n",
    "\n",
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\lstmdata'\n",
    "# test_path = os.path.join(root_captioning,f'pc1.pkl')\n",
    "# if not os.path.exists(test_path):\n",
    "#   with open(test_path, \"wb\") as fp:\n",
    "#     print('dumped')\n",
    "#     pickle.dump(pc1, fp)\n",
    "# else:\n",
    "#   with open(test_path, \"rb\") as fp:\n",
    "#     pc1 = pickle.load(fp)\n",
    "\n",
    "    \n",
    "# test_path = os.path.join(root_captioning,f'pc2.pkl')\n",
    "# if not os.path.exists(test_path):\n",
    "#   with open(test_path, \"wb\") as fp:\n",
    "#     print('dumped')\n",
    "#     pickle.dump(pc2, fp)\n",
    "# else:\n",
    "#   with open(test_path, \"rb\") as fp:\n",
    "#     pc2 = pickle.load(fp)\n",
    "    \n",
    "\n",
    "test_path = os.path.join(root_captioning,f'x_train1000.pkl')\n",
    "if not os.path.exists(test_path):\n",
    "  with open(test_path, \"wb\") as fp:\n",
    "    print('dumped')\n",
    "    pickle.dump(x_train, fp)\n",
    "else:\n",
    "  with open(test_path, \"rb\") as fp:\n",
    "    x_train = pickle.load(fp)\n",
    "\n",
    "test_path = os.path.join(root_captioning,f'y_train1000.pkl')\n",
    "if not os.path.exists(test_path):\n",
    "  with open(test_path, \"wb\") as fp:\n",
    "    print('dumped')\n",
    "    pickle.dump(y_train, fp)\n",
    "else:\n",
    "  with open(test_path, \"rb\") as fp:\n",
    "    y_train = pickle.load(fp)\n",
    "\n",
    "\n",
    "# print(len(pc1))\n",
    "# #print(groundTruth[97])\n",
    "# print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc1, weights=(1, 0, 0, 0)))\n",
    "# print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1947\n",
      "1947\n",
      "972 975\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(x_train))\n",
    "ctone = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 1:\n",
    "        ctone+=1\n",
    "print(ctone, len(y_train) - ctone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajulr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#x_trainScaled = sc_x.fit_transform(x_train)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "np_x_train = np.array(x_train)\n",
    "classifier.fit(np_x_train, np.array(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(np.array(np_x_train))\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9   0.3   0.2   0.3   0.51]]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[545, 430],\n",
       "       [417, 555]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.array([-0.9,0.3,0.2,0.3,0.51])\n",
    "print(sample.reshape(1,-1))\n",
    "\n",
    "y_predsLog=classifier.predict( np.array(x_train))\n",
    "y_predsKMeans = kmeans.labels_\n",
    "#classifier.score( sample.reshape(1,-1))\n",
    "\n",
    "print(y_predsLog)\n",
    "#print(np.array(y_train).reshape(-1,1))\n",
    "confusion_matrix(y_train, y_predsLog)\n",
    "#confusion_matrix(y_train, y_predsKMeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "root_captioning = 'D:\\\\flickr8k\\\\Flickr8k\\\\Flickr8k_Dataset\\\\Flickr8k_Dataset'\n",
    "# print(lookup)\n",
    "pc1=[]\n",
    "pc2=[]\n",
    "pc3=[]\n",
    "pc4=[]\n",
    "groundTruth = []\n",
    "\n",
    "count=0\n",
    "for z in tqdm(range(30,1000)):\n",
    "  count+=1\n",
    "  pic = list(encoding_testlstm.keys())[z]\n",
    "\n",
    "  image = encoding_testlstm[pic].reshape((1,OUTPUT_DIM))\n",
    "#   print(os.path.join(root_captioning, pic))\n",
    "  x=plt.imread(os.path.join(root_captioning, pic))\n",
    "  plt.imshow(x)\n",
    "  plt.show()\n",
    "#   print(\"Caption:\",generateCaptionbs(image))\n",
    "\n",
    "  captionFromKNN = prepKNN(pic)\n",
    "#   print(captionFromKNN)\n",
    "  #cs= generateCaptionBeamSearch(image,30)\n",
    "  cs2=generateCaptionBeamSearchModified(image,3)\n",
    "  #cs3=propModel(cs2, captionFromKNN)\n",
    "\n",
    "  probab = cs2[1]\n",
    "  cs2 = cs2[0]\n",
    "  print( 'lstm', cs2)\n",
    "  print('knn', captionFromKNN[0][0])\n",
    "  pc1.append(captionFromKNN[0][0])\n",
    "  pc2.append(cs2)\n",
    "  \n",
    "  references=[]\n",
    "  score=0\n",
    "  \n",
    "  reference=lookup[pic]\n",
    "  for i in reference:\n",
    "      rs=i.split()\n",
    "      references.append(rs)\n",
    "  logisticFeatures = np.array([probab, captionFromKNN[0][1], captionFromKNN[1], len(captionFromKNN[0][0])/32, len(cs2)/32])\n",
    "  lstmBetter = classifier.predict(logisticFeatures.reshape(1,-1))\n",
    "  if lstmBetter == 1:\n",
    "        print(cs2)\n",
    "        pc3.append(cs2)\n",
    "  else :\n",
    "    print(captionFromKNN[0][0])\n",
    "    pc3.append(captionFromKNN[0][0])\n",
    "        \n",
    "  #put(x_train, y_train, logisticFeatures, lstmBetter, abs(val))\n",
    "    \n",
    "  groundTruth.append(references)  \n",
    "  print(count)\n",
    "\n",
    "  \n",
    "  \n",
    "  print(\"_____________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.560234122477505\n",
      "0.15956231752059055\n",
      "0.5812226391494684\n",
      "0.1601379456957741\n",
      "0.5967312133593888\n",
      "0.1820357248400809\n",
      "0.5698916161938158\n",
      "0.1619807056891054\n"
     ]
    }
   ],
   "source": [
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc1, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc1))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc2, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc2))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc3, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc3))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc4, weights=(1, 0, 0, 0)))\n",
    "print(nltk.translate.bleu_score.corpus_bleu(groundTruth, pc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
